{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5jwqxtGH7JR"
   },
   "source": [
    "### 位置エンコーディングの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja_g99AUIJTF"
   },
   "source": [
    "### Transformerデコーダの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    null_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, img_size: int, length_max: int, dim_embedding: int,\n",
    "                  vocab_size: int, tokenizer, dropout: float=0.1, model_id: str=''):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.max_idx_en = len( tokenizer )\n",
    "\n",
    "        #CLIP\n",
    "        clip_model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(clip_model_id, output_hidden_states = True)\n",
    "        images = torch.randn( ( 1, 3, img_size, img_size ) )\n",
    "        memory = self.clip_model( images )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        self.ln_memory = nn.LayerNorm( dim_embedding )\n",
    "\n",
    "        self.emb = nn.Embedding( vocab_size, dim_embedding, padding_idx=tokenizer.pad_token_id )\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        self.dc_linear = nn.Linear( clip_dim * 3, dim_embedding )\n",
    "        #self.dc_ln = nn.LayerNorm( dim_embedding )\n",
    "\n",
    "        # Down Sampling\n",
    "        #img_length = 577\n",
    "        #length_max = 84\n",
    "        stride = img_length // length_max\n",
    "        self.conv1 = nn.Conv1d( dim_embedding, dim_embedding, 1, stride )\n",
    "        print( \"img_length:\", img_length )\n",
    "        print( \"text_length_max:\", length_max )\n",
    "        print( \"stride:\", stride )\n",
    "        seq_len = self.conv1( memory.transpose(1,2) ).size( 2 )\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        self.ln_length = nn.LayerNorm( dim_embedding )\n",
    "        self.conv_length = nn.Conv1d( seq_len, 1, 1 )\n",
    "        self.embed_lengths = nn.Embedding(1024, dim_embedding)\n",
    "        nn.init.normal_(self.embed_lengths.weight, mean=0, std=0.02)\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "    '''\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor, caption_lengths: torch.Tensor ):\n",
    "\n",
    "        self.device = images.device\n",
    "\n",
    "        masked_captions, mask = self.masking( captions, caption_lengths )\n",
    "        \n",
    "        memory = self.clip_model( images )\n",
    "        memory = self.dense_connector( memory )\n",
    "        memory = self.dropout( memory )\n",
    "        memory = self.ln_memory( memory )\n",
    "\n",
    "        memory = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "        \n",
    "        emb_caption = self.emb( masked_captions ) * math.sqrt(self.dim_embedding)\n",
    "        emb_caption += self.pos_emb( emb_caption )\n",
    "\n",
    "        bert_in = torch.cat( [memory, emb_caption], dim = 1 )\n",
    "        bert_in_padding_masks = (~(torch.eq( masked_captions, self.pad_token_id ))).float()\n",
    "        bert_in_padding_masks = torch.cat( [torch.ones( memory.shape[:2], device=self.device ), bert_in_padding_masks], dim = 1 )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = bert_in, attention_mask = bert_in_padding_masks ).last_hidden_state\n",
    "        outputs = outputs[:,memory.size(1):,:]\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        logits = self.linear( outputs )\n",
    "\n",
    "        predicted_lengths = self.lengths_predictor( memory )\n",
    "        \n",
    "        return logits, mask, predicted_lengths\n",
    "\n",
    "    def dense_connector(self, memory ):\n",
    "        tmp1 = torch.tensor([], device = self.device )\n",
    "        tmp2 = torch.tensor([], device = self.device )\n",
    "        tmp_full = len( memory.hidden_states )\n",
    "        tmp_half = tmp_full // 2\n",
    "        for i in range( 0, tmp_half ):\n",
    "            tmp1 = torch.cat( [tmp1, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp1 = torch.sum(tmp1, dim=0) / tmp_half\n",
    "        for i in range( tmp_half, tmp_full ):\n",
    "            tmp2 = torch.cat( [tmp2, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp2 = torch.sum(tmp2, dim=0 ) / ( tmp_full - tmp_half )\n",
    "        tmp3 = torch.cat([tmp1, tmp2], dim=-1)\n",
    "        tmp3 = torch.cat( [ memory.last_hidden_state, tmp3], dim = -1 )\n",
    "        #tmp3 = sel.dc_ln( tmp3 )\n",
    "        tmp3 = self.dc_linear( tmp3 )\n",
    "        return tmp3\n",
    "\n",
    "    def masking(self, input_x: torch.Tensor, lengths: torch.Tensor) -> tuple[torch.Tensor]:\n",
    "\n",
    "        output = input_x.clone()\n",
    "\n",
    "        masks = torch.zeros_like( output, device=output.device, dtype=torch.bool )       \n",
    "        \n",
    "        #sum_num_mask = 0\n",
    "        #sum_num_arbi = 0\n",
    "        #sum_num_nochange = 0\n",
    "        for n in range( output.size(0) ):\n",
    "            all_prob = torch.rand( (1) )\n",
    "            if all_prob > 0.99:\n",
    "                num_mask = lengths[n]\n",
    "                num_arbi = 0\n",
    "                num_nochange = 0\n",
    "            else:\n",
    "                mask_prob0 = torch.rand( (1) )\n",
    "                mask_prob = all_prob * mask_prob0\n",
    "                resi_prob = all_prob * ( 1.0 - mask_prob0 )\n",
    "                arbi_prob = all_prob * ( resi_prob * 0.5 )\n",
    "                nochange_prob = all_prob * ( resi_prob * 0.5 )\n",
    "                num_mask = math.floor( lengths[n].item() * mask_prob )\n",
    "                num_arbi = math.floor( lengths[n].item() * arbi_prob )\n",
    "                num_nochange = math.floor( lengths[n].item() * nochange_prob )\n",
    "\n",
    "            #sum_num_mask += num_mask\n",
    "            #sum_num_arbi += num_arbi\n",
    "            #sum_num_nochange += num_nochange\n",
    "            \n",
    "            mask_mask = list( random.sample( list(range( 0, lengths[n])),  num_mask ))\n",
    "            output[n,mask_mask] = self.mask_token_id\n",
    "            not_mask_mask = [ n for n in range( lengths[n] ) if n not in mask_mask ]\n",
    "            mask_arbi = random.sample( not_mask_mask, num_arbi )\n",
    "            for i in range( lengths[n] ):\n",
    "                if i in mask_arbi:\n",
    "                    output[n,i] = torch.randint( 0, self.max_idx_en, size=(1,))\n",
    "            not_mask_arbi = [ n for n in not_mask_mask if n not in mask_arbi ]\n",
    "            mask_nochange = random.sample( not_mask_arbi, num_nochange )\n",
    "            not_mask_nochange = [ n for n in not_mask_arbi if n not in mask_nochange ]\n",
    "            mask = [ False if n in not_mask_nochange else True for n in range(lengths[n]) ]\n",
    "            masks[n,:lengths[n]] = torch.tensor( mask )\n",
    "\n",
    "        #print( \"sum_num_mask:\", sum_num_mask )\n",
    "        #print( \"calculate num mask:\", torch.sum( torch.eq( output, self.mask_token_id ).int() ) )\n",
    "        #print( \"sum_num_mask + sum_num_arbi :\", sum_num_mask + sum_num_arbi )\n",
    "        #print( \"num not equal:\", torch.sum( torch.ne( input_x, output ).int() ) )\n",
    "        #print( \"sum_num_mask + sum_num_arbi + sum_nochange:\", sum_num_mask + sum_num_arbi + sum_num_nochange )\n",
    "        #print( \"num of mask True:\", torch.sum( torch.eq( masks, True ) ) )\n",
    "        \n",
    "        return output, masks\n",
    "        \n",
    "    def lengths_predictor(self, memory):\n",
    "        \n",
    "        x = self.ln_length(memory)\n",
    "        x = self.conv_length( x )\n",
    "        #print( \"size of x[:,0,:]:\",x[:,0,:].size())\n",
    "        #print( \"size of self.pos_emb.pos_emb.weight.tranpose(0,1):\",self.pos_emb.pos_emb.weight.transpose(0,1).size())\n",
    "        predicted_lengths_logits = torch.matmul( x[:,0,:], self.embed_lengths.weight.transpose(0,1)).float()\n",
    "        predicted_lengths_logits [:,0] += float('-inf')\n",
    "        predicted_lengths = F.log_softmax( predicted_lengths_logits, dim = -1 )\n",
    "        #predicted_lengths は複数の候補が確率とともに\n",
    "        \n",
    "        return predicted_lengths\n",
    "\n",
    "    def my_decode(self, token_list, tokenizer ):\n",
    "\n",
    "        def my_index( l, x ):\n",
    "            if x in l:\n",
    "                return l.index(x)\n",
    "            else:\n",
    "                return -1\n",
    "        if my_index( token_list, tokenizer.sep_token_id ) != -1:\n",
    "            token_list = token_list[:my_index( token_list, tokenizer.sep_token_id )]\n",
    "        else:\n",
    "            token_list = token_list\n",
    "            \n",
    "        text = tokenizer.decode( token_list, skip_special_tokens = True )\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        self.lengths = []\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "        with open( file_path, \"r\" ) as f:\n",
    "            for i, line in enumerate( f ):\n",
    "                if i % 100000 == 0:\n",
    "                    print( \"i:\", i )\n",
    "                self.img_file.append(line.split(\"\\t\" )[0])\n",
    "                caption = line.split(\"\\t\")[1].replace( \"\\r\\n\", \"\" ).replace( \"\\n\", \"\").replace( \"\\r\", \"\" )\n",
    "                id_tokens = tokenizer.encode( caption )\n",
    "                length_sum += len( id_tokens )\n",
    "                if length_max == None:\n",
    "                    if self.length_max < len( id_tokens ):\n",
    "                        self.length_max = len( id_tokens )\n",
    "                    id_tokens = torch.tensor( id_tokens  )\n",
    "                    self.lengths.append( len( id_tokens ) )\n",
    "                else:\n",
    "                    id_tokens = torch.tensor( id_tokens )[:length_max]\n",
    "                    self.lengths.append( len( id_tokens ) )\n",
    "                \n",
    "                self.tokens.append( id_tokens )\n",
    "\n",
    "                #line = f.readline()\n",
    "        print(\"avg len:\", length_sum / len( self.tokens ) )    \n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        lengths = self.lengths[index]\n",
    "        \n",
    "        return img, tokens, lengths\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index ):\n",
    "    imgs, tokens, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor( lengths )\n",
    "    \n",
    "    max_length = torch.max( lengths )\n",
    "    \n",
    "    targets = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "    \n",
    "    imgs = torch.stack( imgs, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    \n",
    "    return imgs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        self.img_size = 336\n",
    "        self.length_max = 84\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.lr_clip = 2e-7\n",
    "        self.lr_bert = 2e-5            # 学習率\n",
    "        self.lr_others = 1e-4\n",
    "        self.weight_decay = 0.01\n",
    "        self.dropout = 0.1         # dropout確率\n",
    "        self.batch_size = 20       # ミニバッチ数\n",
    "        self.num_epochs = 10       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        self.use_saved_pth = True\n",
    "        self.use_saved_pth = False\n",
    "        self.model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.warmup = 0.1\n",
    "        self.alpha = 0.9\n",
    "        self.betas = (0.9, 0.999)\n",
    "        \n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '../CLIP_LLM_AR/dataset.txt'\n",
    "        self.save_directory = './model'\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        self.num_workers = 0 if self.device == torch.device('cpu') else 12\n",
    "        #self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_length: 577\n",
      "text_length_max: 84\n",
      "stride: 6\n",
      "torch.Size([2, 50, 30522])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "model = CaptioningTransformer(img_size = 336, length_max = 84, dim_embedding=1024, vocab_size=len(tokenizer),\n",
    "                 tokenizer=tokenizer, dropout=0.1, model_id =model_id).to(device)\n",
    "\n",
    "images = torch.randn( ( 2, 3, 336,336 ), device = device )\n",
    "captions = torch.randint( 0, len(tokenizer), size= (2, 50 ), device= device )\n",
    "caption_lengths = torch.randint( 0, 50, size=(2,), device= device )\n",
    "logits, mask, predicted_lengths = model( images, captions, caption_lengths )\n",
    "\n",
    "print( logits.size() )\n",
    "print( mask.size() )\n",
    "print( predicted_lengths.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbR7QGrr5ouJ"
   },
   "source": [
    "### 学習率スケジューラ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_length_loss( predicted_lengths, length_target):\n",
    "    #length_target = target.ne( tokenizer.pad_token_id).sum(-1).unsqueeze(-1)\n",
    "    length_lprobs = predicted_lengths\n",
    "    length_loss = -length_lprobs.gather( dim = -1, index=length_target[:,None])\n",
    "    #length_loss = length_loss.sum()\n",
    "    length_loss = length_loss.float().mean()\n",
    "    return length_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xBOP-3aIHFjB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uchiyats/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 100000\n",
      "i: 200000\n",
      "i: 300000\n",
      "i: 400000\n",
      "i: 500000\n",
      "avg len: 42.0877771734418\n",
      "config.device: cuda:0\n",
      "学習セット数: 20298\n",
      "評価セット数: 2538\n",
      "テストセット数: 50744\n",
      "use_amp: True\n",
      "use_saved_pth: False\n",
      "img_length: 577\n",
      "text_length_max: 84\n",
      "stride: 6\n",
      "exist pth file: True\n",
      "begin_epoch: 0\n",
      "global_ste: 0\n",
      "epochs: 10\n",
      "batch_size: 20\n",
      "num_global_steps: 202980\n",
      "num_warmup_steps: 20298.0\n",
      "train_param: 6766\n",
      "val_param: 846\n",
      "train_loss_file: ./model/MyOriginal_train_loss_20250912_071056.csv\n",
      "lr_clip  : 2e-07\n",
      "lr_bert  : 2e-05\n",
      "lr_others: 0.0001\n",
      "weight_decay: 0.01\n",
      "betas: (0.9, 0.999)\n",
      "alpha: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bf5a48289249f7b955b5b792df628d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uchiyats/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "2025-09-12 07:11:01.805717: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-12 07:11:01.847592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757628661.867580  551107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757628661.873421  551107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757628661.896992  551107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757628661.897014  551107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757628661.897016  551107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757628661.897017  551107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-12 07:11:01.905270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 9.853187506158242e-12\n",
      "lr bert  : 9.853187506158243e-10\n",
      "lr others: 4.9265937530791215e-09\n",
      "Train epoch = 0, loss1 = 10.493376731872559, loss2 = 6.682277202606201, loss = 10.112266540527344, WER = 215.59202575683594, BLEU = 15.343134880065918\n",
      "refe: there are two girl, girls in the picture, holding a microphone in their hands. both are smiling.\n",
      "hypo: proportions [unused199] legislatures pouch casketscoe quantity pouch explainse quantity suicide メ quantity free lash suicide usa 1683 proportions comb falcon pouch boer ⁱ [unused375] suicide versatile vendor fascist lesbian 999ya marx pouch rein pouch [unused771] pouch average divinity traveller pouchbon freekei resumed promptly smirk pouch premiered average suicide internment permits waters ortiz slot casketbon pouch suicide suicide devin devin explainsisches suebon threat boer pouch deeply countriessity bothered pouch 1927 pouch pouch pouch average pouchuka\n",
      "lr clip  : 6.667651985417282e-08\n",
      "lr bert  : 6.667651985417283e-06\n",
      "lr others: 3.333825992708641e-05\n",
      "Train epoch = 0, loss1 = 3.690134286880493, loss2 = 3.983954668045044, loss = 3.7195160388946533, WER = 47.384674072265625, BLEU = 61.67539596557617\n",
      "refe: in this image i can see few tables and on the tables i can see few plates, few glasses, few tissues and few other objects. i can see few chairs which are black in color around the tables. in the background i can see the wall, few photo frames to the walls, few lights, few racks with bottles in it, the curtain and the door.\n",
      "hypo: , the, i the see few tables and on the tables i can see few the, few glasses, the the and few other objects. i can the few chairs which are black in color around the tables. in the, i can see, wall, few the the to the,, few lights the few the with bottles in it, the table and the door.\n",
      "lr clip  : 1.333431865208395e-07\n",
      "lr bert  : 1.333431865208395e-05\n",
      "lr others: 6.667159326041975e-05\n",
      "Train epoch = 0, loss1 = 3.298614501953125, loss2 = 3.93748140335083, loss = 3.3625006675720215, WER = 45.99117660522461, BLEU = 68.45100402832031\n",
      "refe: here in the front we can see three people standing on the floor over there and we can see all of them are smiling and the woman in the front is having a handbag on her and behind them we can see some wooden toys and a portrait present over there and on the right side we can see number of people standing here and there on the floor over there and we can see lights on the roof here and there\n",
      "hypo: here in the front we can see three and standing on the floor over there and we can see all of them are smiling and the woman in the front is having a hand\n",
      "Train epoch = 0, loss1 = 3.0910871028900146, loss2 = 3.9135048389434814, loss = 3.1733286380767822, WER = 36.39457321166992, BLEU = 75.96427917480469\n",
      "refe: in this image, we can see few people. here a person is holding a book. at the bottom, we can see a desk. so many things and objects are placed on it. here there is a chair. background we can see banners, posters, curtain, some objects, wall.\n",
      "hypo: in this image, we can see few people. here a person is holding a book. at the bottom, we can see a desk. so many things and objects are placed on it. here there is a chair. background we can see banners, posters, curtain, some objects, wall.\n",
      "学習率 clip  : 2e-07\n",
      "学習率 bert  : 2e-05\n",
      "Train loss1: 3.091087005138397\n",
      "Train loss2: 3.913505024909973\n",
      "Train loss: 3.1733287286758425\n",
      "Train WER: 36.39457364055374\n",
      "Train BLEU: 75.96428303194183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672fae36b4334d92996e2105f272eaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 0, loss1 = 2.996779203414917, loss2 = 3.88214111328125, loss = 3.085315227508545, WER = 34.0, BLEU = 85.4687271118164\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see in trees few are green in in in in in in in in in in in in the in i can in in in standing, the road in few vehicles in few in, in in and in sky in in in in in in in in in in in in in in in in in in in\n",
      "Val epoch = 0, loss1 = 3.0988852977752686, loss2 = 3.921879291534424, loss = 3.181185007095337, WER = 41.089805603027344, BLEU = 76.60462188720703\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see flowers, where we can see few people and some objects.\n",
      "Val epoch = 0, loss1 = 3.062443733215332, loss2 = 3.8770389556884766, loss = 3.1439034938812256, WER = 41.13410568237305, BLEU = 76.4074935913086\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some in and a in, in the background there are some trees, we can see in in here.\n",
      "Val epoch = 0, loss1 = 3.000610113143921, loss2 = 3.9058356285095215, loss = 3.091132164001465, WER = 39.27800369262695, BLEU = 77.77311706542969\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a a who is is a chop\n",
      "Validation loss1: 3.0006098437309263\n",
      "Validation loss2: 3.9058356881141663\n",
      "Validation WER: 39.27800321182987\n",
      "Validation BLEU: 77.77311979908126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f407014c8f944ddea546bc11b24b96b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 1.9999890520138821e-07\n",
      "lr bert  : 1.9999890520138824e-05\n",
      "lr others: 9.999945260069411e-05\n",
      "Train epoch = 1, loss1 = 2.7560439109802246, loss2 = 3.82975172996521, loss = 2.8634145259857178, WER = 35.45918273925781, BLEU = 73.46664428710938\n",
      "refe: in this image we can see two persons standing near the building, a person is wearing uniform and holding a gun and there are leaves on the floor, there is a railing and trees beside the building.\n",
      "hypo: in this image we can see two persons standing near the building, a person is wearing uniform and holding a in and there are leaves on the floor, there is a railing and trees beside the building.\n",
      "lr clip  : 1.925914977939808e-07\n",
      "lr bert  : 1.9259149779398082e-05\n",
      "lr others: 9.62957488969904e-05\n",
      "Train epoch = 1, loss1 = 2.919363021850586, loss2 = 3.885070562362671, loss = 3.0159335136413574, WER = 32.449703216552734, BLEU = 78.77717590332031\n",
      "refe: in this image i can see the ground, a plant which is green in color, a car which is white in color and a hut. i can see few windows of the hut and i can see the sky in the background.\n",
      "hypo: in this image i can see the ground, a plant which is green in color, in car in is white in color and a i. i can see few windows of the i and i can i i few few the background.\n",
      "lr clip  : 1.8518409038657338e-07\n",
      "lr bert  : 1.851840903865734e-05\n",
      "lr others: 9.25920451932867e-05\n",
      "Train epoch = 1, loss1 = 2.8145241737365723, loss2 = 3.8690428733825684, loss = 2.919975519180298, WER = 30.064550399780273, BLEU = 81.45775604248047\n",
      "refe: in this image i can see a car on the road. one person is wearing a helmet and sitting in the car. on both sides of the road i can see the grass.\n",
      "hypo: in this image i can see the car on the the. one the is wearing see see see see in see see. see see sides see the see i can see see grass.\n",
      "Train epoch = 1, loss1 = 2.74281644821167, loss2 = 3.867774248123169, loss = 2.8553121089935303, WER = 28.695226669311523, BLEU = 83.10071563720703\n",
      "refe: in this image i can see a keyboard, a monitor and a laptop on the table.\n",
      "hypo: in this image i can see a keyboard,.. racks a laptop on. table.\n",
      "学習率 clip  : 1.7777777777777776e-07\n",
      "学習率 bert  : 1.7777777777777777e-05\n",
      "Train loss1: 2.7428165435791017\n",
      "Train loss2: 3.867774639129639\n",
      "Train loss: 2.8553122878074646\n",
      "Train WER: 28.695229876610487\n",
      "Train BLEU: 83.10071230552714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4109982e67814ac4b2742ba6fd409f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 1, loss1 = 3.0301871299743652, loss2 = 3.896893262863159, loss = 3.1168575286865234, WER = 17.375, BLEU = 94.4908447265625\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few sky, few buildings, sky trees and the sky.\n",
      "Val epoch = 1, loss1 = 2.685990571975708, loss2 = 3.888770341873169, loss = 2.8062682151794434, WER = 27.796985626220703, BLEU = 84.00509643554688\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see trees few where we can see few people and some objects.\n",
      "Val epoch = 1, loss1 = 2.671024799346924, loss2 = 3.844860792160034, loss = 2.7884082794189453, WER = 26.785934448242188, BLEU = 84.57442474365234\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some in and a in,, the background in in some trees, in in see see see idol.\n",
      "Val epoch = 1, loss1 = 2.6652672290802, loss2 = 3.8763089179992676, loss = 2.7863714694976807, WER = 27.60863494873047, BLEU = 83.65000915527344\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i can see a man a a a a a a a a a a a a a a a a a a,, the the the the the the the the the the the the the the the the..\n",
      "Validation loss1: 2.6652674198150637\n",
      "Validation loss2: 3.8763092613220214\n",
      "Validation WER: 27.60863580886667\n",
      "Validation BLEU: 83.65000902366124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68566ef6e5cd413f87673b37a81fbfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 1.7777668297916596e-07\n",
      "lr bert  : 1.77776682979166e-05\n",
      "lr others: 8.8888341489583e-05\n",
      "Train epoch = 2, loss1 = 2.8044729232788086, loss2 = 4.008514404296875, loss = 2.924877166748047, WER = 32.8021240234375, BLEU = 83.9969482421875\n",
      "refe: in this image we can see a monument. we can also see some trees, wall and the sky.\n",
      "hypo: in this image we can see a monument. we can also see some trees, can and the sky can\n",
      "lr clip  : 1.7036927557175857e-07\n",
      "lr bert  : 1.703692755717586e-05\n",
      "lr others: 8.51846377858793e-05\n",
      "Train epoch = 2, loss1 = 2.587308406829834, loss2 = 3.872014045715332, loss = 2.7157790660858154, WER = 27.238988876342773, BLEU = 84.21029663085938\n",
      "refe: in this image we can see a person. the background of the image is blurred. to the right side of the image there is another person.\n",
      "hypo: in this image we can see a person. the background of the image is blurred. to the right side of the image there is the person.\n",
      "lr clip  : 1.6296186816435115e-07\n",
      "lr bert  : 1.629618681643512e-05\n",
      "lr others: 8.148093408217558e-05\n",
      "Train epoch = 2, loss1 = 2.5247137546539307, loss2 = 3.872509717941284, loss = 2.6594934463500977, WER = 26.198505401611328, BLEU = 85.73295593261719\n",
      "refe: in this image there are 2 persons playing a musical instrument holding in their hands.\n",
      "hypo: in this image there are 2 persons playing a violin instrument holding violin are violin.\n",
      "Train epoch = 2, loss1 = 2.444925308227539, loss2 = 3.867002487182617, loss = 2.5871329307556152, WER = 25.826915740966797, BLEU = 86.41191101074219\n",
      "refe: in this image we can see people standing on the floor by holding beverage tumblers in their hands. in the background there are people sitting on the chairs and tables are placed in front of them. on the tables there are glass tumblers. in addition to this we can see electric lights, decors and walls.\n",
      "hypo: in this image we can see persons standing on the floor. holding beverage theirrs in their hands. in the background there are people sitting on the chairs and tables are placed in front of them. on the tables there. glass theirrs. in. to this we can see electric lights,.s and walls.\n",
      "学習率 clip  : 1.5555555555555556e-07\n",
      "学習率 bert  : 1.555555555555556e-05\n",
      "Train loss1: 2.4449252104759216\n",
      "Train loss2: 3.867002685070038\n",
      "Train loss: 2.5871328997611998\n",
      "Train WER: 25.82691676735265\n",
      "Train BLEU: 86.41191139364649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124b7bf072e140e9bed53487059f69d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 2, loss1 = 2.3117377758026123, loss2 = 3.830548048019409, loss = 2.463618755340576, WER = 28.375, BLEU = 83.69453430175781\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "Val epoch = 2, loss1 = 2.3777832984924316, loss2 = 3.8754239082336426, loss = 2.5275473594665527, WER = 23.898252487182617, BLEU = 88.36492919921875\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see plants, where we can see few people and we objects.\n",
      "Val epoch = 2, loss1 = 2.3690338134765625, loss2 = 3.8282482624053955, loss = 2.5149550437927246, WER = 24.839170455932617, BLEU = 87.66374206542969\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some graveyard, a graveyard, in the background there are some trees can we can see can background plants.\n",
      "Val epoch = 2, loss1 = 2.3063805103302, loss2 = 3.8599743843078613, loss = 2.461740016937256, WER = 23.684894561767578, BLEU = 88.19485473632812\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can on see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "Validation loss1: 2.3063805675506592\n",
      "Validation loss2: 3.859974458217621\n",
      "Validation WER: 23.684893146594717\n",
      "Validation BLEU: 88.19485266927671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8bc3ba0f0244e2bbb55122bb46480e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 1.5555446075694376e-07\n",
      "lr bert  : 1.5555446075694377e-05\n",
      "lr others: 7.777723037847188e-05\n",
      "Train epoch = 3, loss1 = 2.3504271507263184, loss2 = 3.7467422485351562, loss = 2.490058660507202, WER = 17.780580520629883, BLEU = 94.62484741210938\n",
      "refe: in the picture i can see screws are fixed to some surface, here i can see a box on which i can see some text. they are placed on the white color surface.\n",
      "hypo: in the picture i can see is are i to some surface, here i can see a box on which i can see some text. i are placed on the white color surface.\n",
      "lr clip  : 1.4814705334953634e-07\n",
      "lr bert  : 1.4814705334953637e-05\n",
      "lr others: 7.407352667476818e-05\n",
      "Train epoch = 3, loss1 = 2.328052282333374, loss2 = 3.8555941581726074, loss = 2.480806350708008, WER = 24.8011531829834, BLEU = 88.0846939086914\n",
      "refe: on the left side, there are three children and a woman sitting. on the right side, there is a girl holding a towel, there is grass and a pipe. in the background, there are plants, a wall of a building which is having a door.\n",
      "hypo: on the left side, there are three children and sitting woman a. on the right side, there is a girl holding a towel, there is in and a pipe. in the background, there are there, there wall of a there which is having a door.\n",
      "lr clip  : 1.4073964594212895e-07\n",
      "lr bert  : 1.4073964594212897e-05\n",
      "lr others: 7.036982297106448e-05\n",
      "Train epoch = 3, loss1 = 2.3125360012054443, loss2 = 3.8637001514434814, loss = 2.4676525592803955, WER = 23.924091339111328, BLEU = 89.22499084472656\n",
      "refe: in this image, we can see people and in the background, there is a vehicle on the road and we can see some poles, agate, some stones, pillars and some other people and there is a wall and we can see a rope on the ground.\n",
      "hypo: in this image, we can see people and there standing there, there is a vehicle see see road and we see see some poles, see see, see stones see see and some see see and there is a see and we can see a rope see see ground.\n",
      "Train epoch = 3, loss1 = 2.1930477619171143, loss2 = 3.8567686080932617, loss = 2.359419584274292, WER = 22.308330535888672, BLEU = 90.65960693359375\n",
      "refe: in this image there are few vehicles, persons, tables, chairs, trees, street lights, a building, a pole attached the box and the sky.\n",
      "hypo: in this image there are few vehicles, persons, tables, chairs, trees, street,,, building, a and attached the box to the. to\n",
      "学習率 clip  : 1.333333333333333e-07\n",
      "学習率 bert  : 1.3333333333333333e-05\n",
      "Train loss1: 2.1930476117134092\n",
      "Train loss2: 3.856768736839294\n",
      "Train loss: 2.359419665336609\n",
      "Train WER: 22.308329652834786\n",
      "Train BLEU: 90.65960446635282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093ed81079c34295b373c258c62e2fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 3, loss1 = 2.058337688446045, loss2 = 3.7990500926971436, loss = 2.2324090003967285, WER = 22.125, BLEU = 86.36640930175781\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color, which flowers which are which in color color in the background i can see few,,,,,, few vehicles, few buildings and few and the the sky.\n",
      "Val epoch = 3, loss1 = 2.0392348766326904, loss2 = 3.8754994869232178, loss = 2.2228610515594482, WER = 21.619733810424805, BLEU = 91.38951110839844\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see plants, where we can see few people people people camera\n",
      "Val epoch = 3, loss1 = 2.051619052886963, loss2 = 3.8333709239959717, loss = 2.2297940254211426, WER = 20.834482192993164, BLEU = 92.25003051757812\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some graves and a graveyard, in the background there are some trees, we can see grass here here.\n",
      "Val epoch = 3, loss1 = 2.041520595550537, loss2 = 3.8638806343078613, loss = 2.2237563133239746, WER = 20.71637535095215, BLEU = 91.92354583740234\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is wearing holding chop and and and a spoon and and is in and in in in in in in there is a is which of full in food of and table and in is in in his in.\n",
      "Validation loss1: 2.0415204262733457\n",
      "Validation loss2: 3.8638803458213804\n",
      "Validation WER: 20.716373813614965\n",
      "Validation BLEU: 91.92354646314416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c932a94eb0ae4155910995775b137ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 1.3333223853472153e-07\n",
      "lr bert  : 1.3333223853472155e-05\n",
      "lr others: 6.666611926736077e-05\n",
      "Train epoch = 4, loss1 = 2.2062149047851562, loss2 = 3.868011474609375, loss = 2.372394561767578, WER = 17.129629135131836, BLEU = 95.42546844482422\n",
      "refe: at the bottom of this image, there are buildings having windows, a tower and trees. in the background, there is blue sky.\n",
      "hypo: at the bottom of this image, there are buildings having windows, a tower and trees. in the background, there is blue sky.\n",
      "lr clip  : 1.2592483112731414e-07\n",
      "lr bert  : 1.2592483112731415e-05\n",
      "lr others: 6.296241556365708e-05\n",
      "Train epoch = 4, loss1 = 2.1491312980651855, loss2 = 3.8783621788024902, loss = 2.322054386138916, WER = 22.641279220581055, BLEU = 90.88306427001953\n",
      "refe: in this image we can see trees with flowers and at the bottom there is an object. in the background we can see clouds in the sky.\n",
      "hypo: in this image we can see trees with flowers and at the bottom there is an object. in the background we can see clouds in the sky.\n",
      "lr clip  : 1.1851742371990672e-07\n",
      "lr bert  : 1.1851742371990674e-05\n",
      "lr others: 5.925871185995336e-05\n",
      "Train epoch = 4, loss1 = 2.04803466796875, loss2 = 3.8667221069335938, loss = 2.22990345954895, WER = 21.36297035217285, BLEU = 92.08236694335938\n",
      "refe: this picture is clicked outside. in the center we can see a car parked on the ground and we can see the two persons seems to be walking on the ground and we can see the metal rods, rocks, green grass, water body, trees and we can see the sky, cables and many other objects and we can see the watermarks on the image.\n",
      "hypo: this picture is clicked outside. in the center we can see a car parked on the ground and we can see the two persons seems to be walking on the ground and we can see the metal rods, rocks, green grass, water body, trees and we can see the sky, cables and many other objects and we can see the watermarks on the..\n",
      "Train epoch = 4, loss1 = 1.9767695665359497, loss2 = 3.8775548934936523, loss = 2.1668481826782227, WER = 21.182723999023438, BLEU = 92.78744506835938\n",
      "refe: in this image there is tree and moon in the sky.\n",
      "hypo: in this image there is tree and and and the sky.\n",
      "学習率 clip  : 1.1111111111111111e-07\n",
      "学習率 bert  : 1.1111111111111113e-05\n",
      "Train loss1: 1.9767695462703705\n",
      "Train loss2: 3.8775549054145815\n",
      "Train loss: 2.166848034858704\n",
      "Train WER: 21.182723851099375\n",
      "Train BLEU: 92.78743659701358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e58bdc09d64d3ab956a411354c8db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 4, loss1 = 1.8509347438812256, loss2 = 3.8439412117004395, loss = 2.0502352714538574, WER = 20.875, BLEU = 92.80429077148438\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few plants which are green green color color the which which are green green green in in the background i can see a person standing a, the, few vehicles the few few few few the and the sky.\n",
      "Val epoch = 4, loss1 = 1.7838599681854248, loss2 = 3.8732187747955322, loss = 1.992795705795288, WER = 18.457826614379883, BLEU = 94.2724380493164\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see planets, where we can see few people and some objects.\n",
      "Val epoch = 4, loss1 = 1.8204185962677002, loss2 = 3.837341070175171, loss = 2.0221107006073, WER = 19.001914978027344, BLEU = 94.46752166748047\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some sculptures and some a, in the background there are some trees, we can see plants symbols here.\n",
      "Val epoch = 4, loss1 = 1.7452342510223389, loss2 = 3.8551297187805176, loss = 1.956223726272583, WER = 17.574037551879883, BLEU = 95.43151092529297\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is holding a chop is and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is a in his hands.\n",
      "Validation loss1: 1.7452342879772187\n",
      "Validation loss2: 3.855129759311676\n",
      "Validation WER: 17.574036586559465\n",
      "Validation BLEU: 95.43152179211937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e2311611584b2eb162a52e04181bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 1.1111001631249932e-07\n",
      "lr bert  : 1.1111001631249934e-05\n",
      "lr others: 5.555500815624967e-05\n",
      "Train epoch = 5, loss1 = 1.9714245796203613, loss2 = 3.6969246864318848, loss = 2.143974542617798, WER = 24.120603561401367, BLEU = 93.62443542480469\n",
      "refe: it is a tilted image, there is a club and it is closed there are some other buildings around the club and beside the the wall of the club a car is parked and in the front there is a lot of grass and some flower plants, on the right side there is a tree.\n",
      "hypo: this is a edited image there there is a building and on the building there are some and on and the. building beside the the there there building there a there is there and there there there there is a there the grass the the flower plants the the the the side the is is tree\n",
      "lr clip  : 1.037026089050919e-07\n",
      "lr bert  : 1.0370260890509192e-05\n",
      "lr others: 5.185130445254596e-05\n",
      "Train epoch = 5, loss1 = 1.9083539247512817, loss2 = 3.8496744632720947, loss = 2.1024858951568604, WER = 19.76403045654297, BLEU = 93.56104278564453\n",
      "refe: this image there is a person walking on the floor. behind him there is a wall. there is some water on the floor. top of image there is wall.\n",
      "hypo: in this a black and white black.. the the the the the the the the the the the the the the the the the the see see see the wall.\n",
      "lr clip  : 9.62952014976845e-08\n",
      "lr bert  : 9.62952014976845e-06\n",
      "lr others: 4.814760074884225e-05\n",
      "Train epoch = 5, loss1 = 1.8692295551300049, loss2 = 3.85270357131958, loss = 2.0675766468048096, WER = 19.340608596801758, BLEU = 94.17378997802734\n",
      "refe: on the right there is a man who is wearing t - shirt, trouser and watch. he is standing near to the table. on the table we can see cardboard. on that we can see some photos and papers are attached to it. in the back we can see window, doors and banners. on the top there is a sky. here we can see trees. on the left there is a speaker\n",
      "hypo: on the right there is a man who is wearing t - shirt, trouser and watch. he is standing near to the table. on the table we can see cardboard. the the we can see some.. papers are the. it. in the back. can see. we doors and we we on the top there is a sky. here. can see trees. on a\n",
      "Train epoch = 5, loss1 = 1.7634153366088867, loss2 = 3.8428006172180176, loss = 1.9713537693023682, WER = 18.275562286376953, BLEU = 94.8710708618164\n",
      "refe: in this image i see the table on which there are white plates on which there is food which is of cream, brown and white in color and i see 2 white color cups and i see bottles over here and i see the white and blue color cloth over here and i see a person over here and i see a white cup in which there is white color cream.\n",
      "hypo: in this image i see the table on which there are white plates on which there are food which is of cream and brown and white in i and i see 2 white and cups and i and bottles over here and i see the white see blue color cloth see here and i see a is is here and i see is white cup in which there is white color cream.\n",
      "学習率 clip  : 8.888888888888888e-08\n",
      "学習率 bert  : 8.888888888888888e-06\n",
      "Train loss1: 1.76341539144516\n",
      "Train loss2: 3.8428002834320067\n",
      "Train loss: 1.9713538527488708\n",
      "Train WER: 18.275562265133125\n",
      "Train BLEU: 94.87106954634842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55d3f012c8b4764b4c5589ac14729c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 5, loss1 = 1.3582632541656494, loss2 = 3.915787935256958, loss = 1.6140156984329224, WER = 16.0, BLEU = 97.04225158691406\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few plants which are green color color, few flowers in in in color color and in the background i can see a few standing, few few, few vehicles, few buildings, few trees and the sky.\n",
      "Val epoch = 5, loss1 = 1.5917391777038574, loss2 = 3.8742291927337646, loss = 1.8199881315231323, WER = 16.461193084716797, BLEU = 96.37561798095703\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see planets, where we can see few people and some objects.\n",
      "Val epoch = 5, loss1 = 1.6000300645828247, loss2 = 3.832028388977051, loss = 1.8232300281524658, WER = 16.44192886352539, BLEU = 96.31089782714844\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some graves on a cross, in the background are there some trees, we can see a plants here.\n",
      "Val epoch = 5, loss1 = 1.584694266319275, loss2 = 3.8565125465393066, loss = 1.8118759393692017, WER = 16.811220169067383, BLEU = 96.41078186035156\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "Validation loss1: 1.5846942394971848\n",
      "Validation loss2: 3.8565120267868043\n",
      "Validation WER: 16.811219720376116\n",
      "Validation BLEU: 96.41077816256126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c8f2f6c4604f0283e27b1465e8c84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 8.888779409027709e-08\n",
      "lr bert  : 8.88877940902771e-06\n",
      "lr others: 4.444389704513855e-05\n",
      "Train epoch = 6, loss1 = 1.6748381853103638, loss2 = 3.7402608394622803, loss = 1.8813804388046265, WER = 14.490674018859863, BLEU = 95.23291778564453\n",
      "refe: in the given image at the bottom i can see some cars, trees, buildings, pole and in the top i can see a black and white picture.\n",
      "hypo: in the given image at the bottom i can see some cars, trees, buildings, pole and in the top i can see a black and white picture.\n",
      "lr clip  : 8.148038668286969e-08\n",
      "lr bert  : 8.148038668286969e-06\n",
      "lr others: 4.074019334143485e-05\n",
      "Train epoch = 6, loss1 = 1.7667077779769897, loss2 = 3.8223254680633545, loss = 1.9722694158554077, WER = 18.148714065551758, BLEU = 95.22734069824219\n",
      "refe: in the picture there are two empty dining tables, on the tables there are flowers vases and candles, in the background there are two windows in between a brick wall.\n",
      "hypo: in this image there are chairs around around tables, on the tables there are flower vases flower flower, there there background\n",
      "lr clip  : 7.407297927546228e-08\n",
      "lr bert  : 7.407297927546229e-06\n",
      "lr others: 3.703648963773114e-05\n",
      "Train epoch = 6, loss1 = 1.702382206916809, loss2 = 3.843686103820801, loss = 1.9165124893188477, WER = 17.66996192932129, BLEU = 95.78083038330078\n",
      "refe: in this image we can see some buildings. we can also see a board with some text on it, some poles, a barricade, a wire, a group of trees and some people standing on the ground which is covered with the snow. on the backside we can see the sky which looks cloudy.\n",
      "hypo: in this image we can see some buildings. we can also see a board with some text on it, somede, a barricade, ade, a group of trees and some people standing on the ground which is covered with the snow. on the backside we can see the sky which looks cloudy.\n",
      "Train epoch = 6, loss1 = 1.6584469079971313, loss2 = 3.815603733062744, loss = 1.8741625547409058, WER = 17.94829559326172, BLEU = 95.60209655761719\n",
      "refe: in this image we can see buildings, poles, lights, traffic signals, and branches. in the background there is sky with clouds.\n",
      "hypo: in this image we can see buildings, poles, lights, traffic signals, and branches. in the background there is sky with clouds.\n",
      "学習率 clip  : 6.666666666666665e-08\n",
      "学習率 bert  : 6.666666666666667e-06\n",
      "Train loss1: 1.6584470868110657\n",
      "Train loss2: 3.8156033325195313\n",
      "Train loss: 1.874162676334381\n",
      "Train WER: 17.948294585397754\n",
      "Train BLEU: 95.60209967749053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0776fc6786464facd3790f5689ae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 6, loss1 = 1.661980390548706, loss2 = 3.87724232673645, loss = 1.8835065364837646, WER = 13.0, BLEU = 101.15412902832031\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color, few flowers which are green in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "Val epoch = 6, loss1 = 1.4860997200012207, loss2 = 3.868082046508789, loss = 1.724298119544983, WER = 15.082554817199707, BLEU = 97.90394592285156\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see planets, where we can see some people and some objects.\n",
      "Val epoch = 6, loss1 = 1.4873486757278442, loss2 = 3.8250012397766113, loss = 1.721113681793213, WER = 15.308104515075684, BLEU = 97.2385025024414\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some graves and a cross, in the background there are some trees, we can see some some here.\n",
      "Val epoch = 6, loss1 = 1.428479790687561, loss2 = 3.853902816772461, loss = 1.6710221767425537, WER = 15.051095962524414, BLEU = 97.59943389892578\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat and and and a on there is a bowl which is on the the the on table and there is a in his mouth.\n",
      "Validation loss1: 1.4284797894954682\n",
      "Validation loss2: 3.8539029216766356\n",
      "Validation WER: 15.05109601514158\n",
      "Validation BLEU: 97.59943421019184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cea606ff35b4a03a5d3b029f3910181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 6.666557186805486e-08\n",
      "lr bert  : 6.666557186805488e-06\n",
      "lr others: 3.333278593402744e-05\n",
      "Train epoch = 7, loss1 = 1.1979515552520752, loss2 = 3.6153886318206787, loss = 1.4396952390670776, WER = 9.516837120056152, BLEU = 102.95063781738281\n",
      "refe: in this picture i can see many birds on the pole and wire. at the bottom i can see roof of the building. at the top i can see the sky.\n",
      "hypo: in this picture i can see many birds on the pole and wire. at the bottom i can see roof of the the. at the top i can see the sky.\n",
      "lr clip  : 5.9258164460647466e-08\n",
      "lr bert  : 5.925816446064747e-06\n",
      "lr others: 2.9629082230323733e-05\n",
      "Train epoch = 7, loss1 = 1.6078873872756958, loss2 = 3.8601551055908203, loss = 1.8331141471862793, WER = 17.090187072753906, BLEU = 96.41214752197266\n",
      "refe: in this image there are two persons in the center the girl is sitting on the chair and is taking her tongue out of the mouth. in the front there is a juice bottle with a name juicy juice at the right side. on the left side there are papers kept on the table, on the left side i can see a hand of the person wearing a brown colour shirt. in the background there are some chairs and\n",
      "hypo: this this image taken is a a the there there the the the there a a the there the there there the a the a the there a there the there the the there the there the a is the the the the the the the the is the there is papers is the the the the the is left the the the the a the the is the wearing a the the the the the the the there are...\n",
      "lr clip  : 5.1850757053240055e-08\n",
      "lr bert  : 5.185075705324006e-06\n",
      "lr others: 2.5925378526620032e-05\n",
      "Train epoch = 7, loss1 = 1.5967555046081543, loss2 = 3.82684326171875, loss = 1.819764256477356, WER = 16.824798583984375, BLEU = 96.55030059814453\n",
      "refe: in the foreground i can see a girl. in the background i can see a fence, pillars, person, grass, trees and the sky. this image is taken may be during a day.\n",
      "hypo: in the foreground i can see a girl. in the background i can see a fence, pillars, person, grass, trees and the sky. this image is taken may be during a day.\n",
      "Train epoch = 7, loss1 = 1.512009859085083, loss2 = 3.8511481285095215, loss = 1.745923638343811, WER = 15.997367858886719, BLEU = 96.90953826904297\n",
      "refe: in this picture there are group of people marching on the road. at the back there are trees. at the top there is sky. at the bottom there is a road and there is ground.\n",
      "hypo: in this picture there are group of people walking on the road. at the back there are trees. at the top there is sky. at the bottom there is a road and there is ground.\n",
      "学習率 clip  : 4.444444444444444e-08\n",
      "学習率 bert  : 4.444444444444444e-06\n",
      "Train loss1: 1.5120098936557769\n",
      "Train loss2: 3.8511483478546142\n",
      "Train loss: 1.7459237146377564\n",
      "Train WER: 15.997369216361356\n",
      "Train BLEU: 96.9095409187465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51a94e3974c4a27becea4079f946dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 7, loss1 = 1.1166132688522339, loss2 = 3.8850319385528564, loss = 1.393455147743225, WER = 11.25, BLEU = 100.99256896972656\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few plants which are green in color and few flowers which are in in color and in the background i can see a person standing the few road, few vehicles, few buildings, few trees and the sky.\n",
      "Val epoch = 7, loss1 = 1.3916603326797485, loss2 = 3.871037483215332, loss = 1.6395981311798096, WER = 14.809781074523926, BLEU = 97.94807434082031\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see a people and we can see few people and a objects.\n",
      "Val epoch = 7, loss1 = 1.389971137046814, loss2 = 3.825019598007202, loss = 1.63347589969635, WER = 13.817962646484375, BLEU = 98.50346374511719\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see a graves graves the here, in the background there are some trees trees trees the down trees trees here.\n",
      "Val epoch = 7, loss1 = 1.337365746498108, loss2 = 3.8515331745147705, loss = 1.5887823104858398, WER = 13.52800178527832, BLEU = 98.75020599365234\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl see is full of food on on table and there is food in his in.\n",
      "Validation loss1: 1.3373656994104386\n",
      "Validation loss2: 3.8515333318710328\n",
      "Validation WER: 13.528001990308447\n",
      "Validation BLEU: 98.75020370413448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3564d138aa5640f5bed0192f35529202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 4.4443349645832644e-08\n",
      "lr bert  : 4.444334964583265e-06\n",
      "lr others: 2.2221674822916323e-05\n",
      "Train epoch = 8, loss1 = 1.3958228826522827, loss2 = 3.5955910682678223, loss = 1.6157996654510498, WER = 11.587301254272461, BLEU = 98.34548950195312\n",
      "refe: in this image in the center there are two persons who are standing and one person is holding a phone and talking, in the background there is a grass and one object. on the right side of the image there is some text written.\n",
      "hypo: in this image in the center there are two persons who are standing and one person is holding a phone and talking, in the background there is a trees and one object. on the right side of the image there some is text written.\n",
      "lr clip  : 3.703594223842524e-08\n",
      "lr bert  : 3.7035942238425245e-06\n",
      "lr others: 1.8517971119212622e-05\n",
      "Train epoch = 8, loss1 = 1.5323857069015503, loss2 = 3.8438777923583984, loss = 1.7635351419448853, WER = 16.39889144897461, BLEU = 96.65179443359375\n",
      "refe: this is image is clicked inside a room, in this image there are four people standing on that on the right there is a man he wear suit t shirt, trouser and shoes. in the middle there is a woman she is smiling her hair is short. in the back ground there are two sofas, plant, curtain, chair and light.\n",
      "hypo: this is image is clicked inside a room, in this image there are four people standing on that on the right there is a man he wear suit t shirt, trouser and shoes. in the middle there is a woman she is smiling her hair is short. in the back ground there are two sofas, plant, curtain, chair and light.\n",
      "lr clip  : 2.962853483101783e-08\n",
      "lr bert  : 2.9628534831017837e-06\n",
      "lr others: 1.4814267415508917e-05\n",
      "Train epoch = 8, loss1 = 1.4700649976730347, loss2 = 3.8377416133880615, loss = 1.7068325281143188, WER = 15.097762107849121, BLEU = 97.96089172363281\n",
      "refe: in this image we can see people standing and holding a board. at the bottom there are stones and grass. in the background there are trees and sky.\n",
      "hypo: in this image we can see people standing and holding a board. at the bottom there is stones and grass. in the background there are trees and sky\n",
      "Train epoch = 8, loss1 = 1.512222409248352, loss2 = 3.833728075027466, loss = 1.7443729639053345, WER = 15.756133079528809, BLEU = 97.37896728515625\n",
      "refe: in this image i can see an object inside the vehicle. to the side i can see the person and few more vehicles. in the background i can see the plants and the building with boards.\n",
      "hypo: in this image i can see an object inside the vehicle. to the side i can see the person and few more vehicles. in the background i can see the plants and the building with boards.\n",
      "学習率 clip  : 2.222222222222222e-08\n",
      "学習率 bert  : 2.222222222222222e-06\n",
      "Train loss1: 1.5122224169969558\n",
      "Train loss2: 3.833728096485138\n",
      "Train loss: 1.7443729507923127\n",
      "Train WER: 15.756133032489247\n",
      "Train BLEU: 97.37896296420988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c339247b8b4353afec4255ee43d51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 8, loss1 = 1.2302350997924805, loss2 = 3.9072036743164062, loss = 1.497931957244873, WER = 13.375, BLEU = 98.37784576416016\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color and few flowers which are red in color and in the color i can see a person standing, the road few few vehicles, few buildings, few trees and the..\n",
      "Val epoch = 8, loss1 = 1.3124650716781616, loss2 = 3.874128818511963, loss = 1.5686312913894653, WER = 13.351112365722656, BLEU = 99.21080017089844\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see planets, where we can see few people and some objects.\n",
      "Val epoch = 8, loss1 = 1.3061429262161255, loss2 = 3.82527232170105, loss = 1.5580557584762573, WER = 13.503962516784668, BLEU = 99.09825134277344\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some statue and a cross, in the background there are some trees, we can see some symbols here.\n",
      "Val epoch = 8, loss1 = 1.3615936040878296, loss2 = 3.8481035232543945, loss = 1.6102443933486938, WER = 14.239962577819824, BLEU = 98.59025573730469\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i see a man who is wearing a chopsticks and a spoon and he is holding a spoon. i can also see there is on there which is and and on on on table on there is is is over in.\n",
      "Validation loss1: 1.3615935498476028\n",
      "Validation loss2: 3.848103380203247\n",
      "Validation WER: 14.239961624522678\n",
      "Validation BLEU: 98.59024269571138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983dd005bf554d8ea1daf6d0520434ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr clip  : 2.2221127423610426e-08\n",
      "lr bert  : 2.222112742361043e-06\n",
      "lr others: 1.1110563711805213e-05\n",
      "Train epoch = 9, loss1 = 1.2345689535140991, loss2 = 3.867771863937378, loss = 1.4978891611099243, WER = 10.823529243469238, BLEU = 102.77295684814453\n",
      "refe: in this image, we can see a man, he is wearing a jacket, the background is not clear.\n",
      "hypo: in this image, we can see a man, he is wearing a jacket, the background is not clear.\n",
      "lr clip  : 1.4813720016203019e-08\n",
      "lr bert  : 1.481372001620302e-06\n",
      "lr others: 7.40686000810151e-06\n",
      "Train epoch = 9, loss1 = 1.4247779846191406, loss2 = 3.8220207691192627, loss = 1.6645022630691528, WER = 15.244206428527832, BLEU = 97.60043334960938\n",
      "refe: the picture is taken on the street of a city. in the center of the picture there are shops, tents, umbrellas, auto rickshaw, motor bike, people and many other objects. in the background there are buildings. in the foreground there are waste papers on the road.\n",
      "hypo: the picture is taken on the streets of a city. in the center of the picture there are shops, tents, umbrellas,,,,, motor bike, people and many other objects. in the background there are buildings. in the foreground there are\n",
      "lr clip  : 7.4063126087956115e-09\n",
      "lr bert  : 7.406312608795613e-07\n",
      "lr others: 3.7031563043978063e-06\n",
      "Train epoch = 9, loss1 = 1.5253695249557495, loss2 = 3.8102896213531494, loss = 1.7538615465164185, WER = 16.316978454589844, BLEU = 97.3529281616211\n",
      "refe: this image having a sofa with the two cushions on it.\n",
      "hypo: this image shows a sofa with a two cushions on it.\n",
      "Train epoch = 9, loss1 = 1.4939767122268677, loss2 = 3.8237838745117188, loss = 1.7269574403762817, WER = 15.490464210510254, BLEU = 97.71900177001953\n",
      "refe: in this image ta the bottom there are some buildings and some statues, on the top of the image there is sky and on the right side there are some trees.\n",
      "hypo: in this image at the bottom there are some buildings and some statues, on the top of the image there is sky and on the right side there are some trees.\n",
      "学習率 clip  : 0.0\n",
      "学習率 bert  : 0.0\n",
      "Train loss1: 1.493976823091507\n",
      "Train loss2: 3.8237838912010194\n",
      "Train loss: 1.7269575035572051\n",
      "Train WER: 15.490463650531376\n",
      "Train BLEU: 97.71900257847028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00edc09188654d969cebad994bda1c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 9, loss1 = 1.3455162048339844, loss2 = 3.873495578765869, loss = 1.5983141660690308, WER = 14.875, BLEU = 99.36988067626953\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see few trees which are green in color color few flowers which are red in color. in the background i can see few persons standing, few, few few vehicles, few buildings and the is the the\n",
      "Val epoch = 9, loss1 = 1.3162145614624023, loss2 = 3.8691415786743164, loss = 1.571507215499878, WER = 13.602813720703125, BLEU = 98.98859405517578\n",
      "refe: in this picture we can see planets, where we can see few people and some objects.\n",
      "hypo: in this picture we can see plants plants here we can see some people and some objects.\n",
      "Val epoch = 9, loss1 = 1.2897738218307495, loss2 = 3.825502634048462, loss = 1.5433465242385864, WER = 13.2413330078125, BLEU = 99.35865020751953\n",
      "refe: in this picture we can see some graves and a memorial, in the background there are some trees, we can see christianity symbols here.\n",
      "hypo: in this picture we can see some graves and a man, in the background there are some trees, we can see some symbols here.\n",
      "Val epoch = 9, loss1 = 1.2233343124389648, loss2 = 3.8512701988220215, loss = 1.4861279726028442, WER = 12.963152885437012, BLEU = 99.12146759033203\n",
      "refe: in this image i see a man who is holding a chopsticks and a spoon and he is wearing a hat. i can also see there is a bowl which is full of food on the table and there is food in his mouth.\n",
      "hypo: in this image i can see a man is holding a chopstick and and and and a a a a in the. i can see a bowl and and i and the couple of food on the table and is a in. his..\n",
      "Validation loss1: 1.2233342814445496\n",
      "Validation loss2: 3.8512705111503602\n",
      "Validation WER: 12.96315480560208\n",
      "Validation BLEU: 99.12146237885487\n"
     ]
    }
   ],
   "source": [
    "config = ConfigTrain()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model_id)\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms,tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=1,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "print( \"config.device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "\n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size, config.length_max,\n",
    "    config.dim_embedding, vocab_size,\n",
    "    tokenizer, config.dropout, config.model_id)\n",
    "model.to(config.device) \n",
    "\n",
    "PATH = \"model/model_bert_mask_curr.pth\"\n",
    "print( \"exist pth file:\", os.path.isfile(PATH) )\n",
    "use_saved_pth = config.use_saved_pth\n",
    "if use_saved_pth and os.path.isfile(PATH):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    begin_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    global_step = checkpoint['global_step']    \n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_ste:\", global_step )\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss( ignore_index = tokenizer.pad_token_id, reduction = 'mean' )\n",
    "\n",
    "params_clip = []\n",
    "params_bert = []\n",
    "params_others = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            params_clip.append(parameter)\n",
    "        elif 'bert' in name:\n",
    "            params_bert.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    {'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_bert, 'lr': config.lr_bert},\n",
    "    {'params': params_others, 'lr': config.lr_others}\n",
    "]\n",
    "\n",
    "# 最適化手法の定義\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas= config.betas )\n",
    "\n",
    "# 全ステップ数\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * config.warmup\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "#スケジューラーの定義\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )    \n",
    "\n",
    "len_tr_loader = len( train_loader )\n",
    "train_param = len_tr_loader // 3\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "val_param = len_val_loader // 3\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f) \n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "print( \"lr_clip  :\", config.lr_clip)\n",
    "print( \"lr_bert  :\", config.lr_bert )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "print( \"alpha:\", config.alpha )\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses1 = deque()\n",
    "        train_losses2 = deque()\n",
    "        train_losses = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            caption_lengths = caption_lengths.to(config.device)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                outputs, mask, predicted_lengths = model( imgs, captions, caption_lengths )\n",
    "\n",
    "                # 損失の計算\n",
    "                # 単語軸が第1軸である必要があるため、転置\n",
    "                loss1 = criterion(outputs[mask], captions[mask])\n",
    "                loss2 = calc_length_loss( predicted_lengths, caption_lengths )\n",
    "                loss = config.alpha * loss1 + ( 1 - config.alpha ) * loss2\n",
    "            \n",
    "            hypo_ids = torch.argmax( outputs, dim = 2 )\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            #scaler.unscale_(optimizer)\n",
    "            #clip_grad_threshold = 5.0\n",
    "            #torch.nn.utils.clip_grad_norm_(\\\n",
    "            #        model.parameters(),\n",
    "            #        clip_grad_threshold)\n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            #for name, param in model.named_parameters():\n",
    "            #    print( name )\n",
    "            \n",
    "            norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm1 = torch.sqrt( torch.norm( model.bert.encoder.layer[23].attention.self.query.weight.grad, p = 2 ) ).item()\n",
    "            norm_mean = torch.mean( torch.stack ([ torch.sqrt( torch.norm( param.grad, p = 2 ) ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] ) ).item()\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm_mean:\", norm_mean, file=f  )\n",
    "                f.flush()\n",
    "            global_step += 1\n",
    "\n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            for (hypo_id, caption) in zip( hypo_ids, captions ):\n",
    "                hypo = model.my_decode( hypo_id.tolist(), tokenizer )\n",
    "                hypo_tokens = tokenizer.tokenize( hypo )\n",
    "                reference = model.my_decode( caption.tolist(), tokenizer )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "                        \n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                    levenshtein.calculate_error(hypo_tokens,\n",
    "                                                    ref_tokens)\n",
    "                \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "        \n",
    "                total_bleu += bleu                    \n",
    "                    \n",
    "                if n < 1 and n_batch == len( train_loader ) - 1 :\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                if n < 1 and n_batch % train_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "            \n",
    "            avg_error = total_error / total_token_length * 100\n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "                \n",
    "            # 学習時の損失をログに書き込み\n",
    "            train_losses1.append(loss1.item())\n",
    "            train_losses2.append(loss2.item())\n",
    "            train_losses.append(loss.item())\n",
    "            train_errors.append( avg_error )\n",
    "            train_bleus.append( avg_bleu )\n",
    "            #train_ciders.append( avg_cider )\n",
    "            if len(train_losses) > config.moving_avg:\n",
    "                train_losses1.popleft()\n",
    "                train_losses2.popleft()\n",
    "                train_losses.popleft()\n",
    "                train_errors.popleft()\n",
    "                train_bleus.popleft()\n",
    "                #train_ciders.popleft()\n",
    "            mean_loss1 = torch.Tensor(train_losses1).mean().item()\n",
    "            mean_loss2 = torch.Tensor(train_losses2).mean().item()\n",
    "            mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "            mean_error = torch.Tensor(train_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss1': mean_loss1,\n",
    "                'loss2': mean_loss2,\n",
    "                'loss': mean_loss,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "                #'CIDER': torch.Tensor(train_ciders).mean().item()\n",
    "            })\n",
    "            with open(train_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_loss1}, {mean_loss2}, {mean_loss}, {mean_error}, {mean_bleu}', file=f)\n",
    "            print_flag = 1\n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                if print_flag == 1:\n",
    "                    print( \"lr clip  :\", optimizer.param_groups[0][\"lr\"] )\n",
    "                    print( \"lr bert  :\", optimizer.param_groups[1][\"lr\"] )\n",
    "                    print( \"lr others:\", optimizer.param_groups[2][\"lr\"] )\n",
    "                    print_flag = 0\n",
    "                #print(f'Train epoch = {epoch}, loss = {loss.item()}, WER = {avg_error}, BLEU = {avg_bleu}, CIDER = {avg_cider}')\n",
    "                print(f'Train epoch = {epoch}, loss1 = {mean_loss1}, loss2 = {mean_loss2}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Train epoch = {epoch}, loss1 = {mean_loss1}, loss2 = {mean_loss2}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                #print(f'Train epoch = {epoch}, loss = {loss.item()}, WER = {avg_error}, BLEU = {avg_bleu}, CIDER = {avg_cider}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "    # 学習率を表示\n",
    "    print(f'学習率 clip  : {optimizer.param_groups[0]['lr']}')\n",
    "    print(f'学習率 bert  : {optimizer.param_groups[1]['lr']}')\n",
    "    print(f'学習率 others: {optimizer.param_groups[2]['lr']}')\n",
    "    train_loss1 = np.mean(train_losses1)\n",
    "    train_loss2 = np.mean(train_losses2)\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_error = np.mean(train_errors )\n",
    "    train_bleu = np.mean(train_bleus )\n",
    "    print(f'Train loss1: {train_loss1}')\n",
    "    print(f'Train loss2: {train_loss2}')\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train WER: {train_error}')        \n",
    "    print(f'Train BLEU: {train_bleu}')\n",
    "\n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "\n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "\n",
    "        #val_losses = []\n",
    "        val_losses1 = deque()\n",
    "        val_losses2 = deque()\n",
    "        val_losses = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            caption_lengths = caption_lengths.to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                outputs, mask, predicted_lengths = model( imgs, captions, caption_lengths )\n",
    "                hypo_ids = torch.argmax( outputs, dim = 2 )\n",
    "                loss1 = criterion(outputs[mask], captions[mask])\n",
    "                loss2 = calc_length_loss( predicted_lengths, caption_lengths )\n",
    "                loss = config.alpha * loss1 + ( 1 - config.alpha ) * loss2\n",
    "            \n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            for (hypo_id, caption) in zip( hypo_ids, captions ):\n",
    "                hypo = model.my_decode( hypo_id.tolist(), tokenizer )\n",
    "                hypo_tokens = tokenizer.tokenize( hypo )\n",
    "                reference = model.my_decode( caption.tolist(), tokenizer )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "                        \n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                    levenshtein.calculate_error(hypo_tokens,\n",
    "                                                ref_tokens)\n",
    "                    \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "        \n",
    "                total_bleu += bleu\n",
    "\n",
    "                if n < 1 and n_batch == len( val_loader ) - 1:\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                        \n",
    "                if n < 1 and n_batch % val_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "                \n",
    "            avg_error = total_error / total_token_length * 100                    \n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "\n",
    "            # 学習時の損失をログに書き込み\n",
    "            val_losses1.append(loss1.item())\n",
    "            val_losses2.append(loss2.item())\n",
    "            val_losses.append(loss.item())\n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            if len(val_losses) > config.moving_avg:\n",
    "                val_losses1.popleft()\n",
    "                val_losses2.popleft()\n",
    "                val_losses.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "            mean_loss1 = torch.Tensor(val_losses1).mean().item()\n",
    "            mean_loss2 = torch.Tensor(val_losses2).mean().item()\n",
    "            mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss1': mean_loss1,\n",
    "                'loss2': mean_loss2,\n",
    "                'loss': mean_loss,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_loss1}, {mean_loss2}, {mean_loss}, {mean_error}, {mean_bleu}', file=f)\n",
    "\n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, loss1 = {mean_loss1}, loss2 = {mean_loss2}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Val epoch = {epoch}, loss1 = {mean_loss1}, loss2 = {mean_loss2}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "    # Loss 表示\n",
    "    val_loss1 = np.mean(val_losses1)\n",
    "    val_loss2 = np.mean(val_losses2)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_error = np.mean( val_errors )\n",
    "    val_bleu = np.mean( val_bleus )\n",
    "    print(f'Validation loss1: {val_loss1}')\n",
    "    print(f'Validation loss2: {val_loss2}')\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'Validation WER: {val_error}')\n",
    "    print(f'Validation BLEU: {val_bleu}')\n",
    "\n",
    "    # より良い検証結果が得られた場合、モデルを保存\n",
    "    if val_loss < val_loss_best:\n",
    "        val_loss_best = val_loss\n",
    "\n",
    "        # モデルを保存\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,},\n",
    "            f'{config.save_directory}/model_bert_mask_best.pth')\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,},\n",
    "        f'{config.save_directory}/model_bert_mask_curr.pth')\n",
    "        \n",
    "# モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': loss,},\n",
    "    f'{config.save_directory}/model_bert_mask_final.pth')\n",
    "#f_norm.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
